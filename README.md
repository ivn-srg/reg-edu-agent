## RAG-EDU Agent (Local)

Локальный RAG-агент для курса «Хранение данных и Введение в Машинное обучение».
Агент отвечает ТОЛЬКО по материалам в папке `data/`, умеет:
- отвечать на вопросы в рамках материалов;
- задавать вопросы для проверки знаний (квизы);
- составлять задания по обсуждаемой теме;
— все строго в рамках предоставленных материалов (DOCX/PDF).

### Быстрый старт
1) Подготовьте Python 3.10+ и виртуальное окружение.
2) Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```
3) Поместите лекции в `data/` (форматы: `.pdf`, `.docx`).
4) Постройте векторное хранилище:
   ```bash
   python -m src.cli ingest
   ```
5) Задайте вопрос:
   ```bash
   python -m src.cli ask "Что такое нормализация данных?"
   ```
6) Сгенерируйте квиз по теме:
   ```bash
   python -m src.cli quiz --topic "Хранение данных" --num 5
   ```
7) Сгенерируйте задание:
   ```bash
   python -m src.cli task --topic "Введение в Машинное обучение"
   ```

### Запуск локального API
```bash
uvicorn src.server:app --host 0.0.0.0 --port 8000
```

Эндпоинты:
- `POST /ingest` — перестроить индекс из `data/`
- `POST /ask` — вопрос-ответ по материалам (с поддержкой истории диалога)
- `POST /quiz` — генерация квиза (с поддержкой истории диалога)
- `POST /task` — генерация задания (с поддержкой истории диалога)

**Эндпоинты для работы с историей диалогов:**
- `POST /conversations` — создание нового диалога
- `GET /conversations/{id}` — получение диалога по ID
- `GET /users/{user_id}/conversations` — список всех диалогов пользователя
- `DELETE /conversations/{id}` — удаление диалога
- `POST /messages` — добавление сообщения в диалог
- `GET /conversations/{id}/messages` — получение всех сообщений диалога
- `PUT /conversations/{id}/title` — обновление заголовка диалога

### Запуск Frontend приложения

1. Перейдите в директорию frontend:
```bash
cd frontend
```

2. Установите зависимости:
```bash
npm install
```

3. Создайте файл `.env` и укажите URL API:
```bash
VITE_API_URL=http://localhost:8000
```

4. Запустите dev сервер:
```bash
npm run dev
```

5. Откройте браузер по адресу `http://localhost:5173`

### Особенности Frontend

- ✅ Адаптивный интерфейс для десктопа и мобильных устройств
- ✅ Поддержка тёмной темы
- ✅ Три типа запросов: вопросы, квизы, задания
- ✅ Сохранение контекста диалога для каждого типа запроса
- ✅ Анимации сообщений с помощью Framer Motion
- ✅ Typing индикатор во время генерации ответа
- ✅ Tooltip с временем отправки сообщений
- ✅ **История диалогов** — автоматическое сохранение всех переписок в базу данных
- ✅ **Боковая панель** — просмотр, загрузка и удаление сохраненных диалогов
- ✅ **Персистентность** — диалоги сохраняются между сессиями

Технологии: React 19, TypeScript, Vite, Tailwind CSS, Framer Motion, Zustand, SQLite

### История диалогов

Все переписки с ботом автоматически сохраняются в базу данных SQLite (`conversations.db`). Функции:

- **Автосохранение**: Каждое сообщение автоматически сохраняется в БД
- **Боковая панель**: Просмотр всех сохраненных диалогов
- **Загрузка диалогов**: Продолжение любого предыдущего диалога
- **Удаление**: Возможность удалить ненужные диалоги
- **Уникальный User ID**: Автоматическая генерация ID для каждого пользователя

Подробнее см. [CONVERSATION_HISTORY.md](CONVERSATION_HISTORY.md)

### Конфигурация LLM
По умолчанию используется Ollama.
Задайте переменные окружения (создайте `.env` при необходимости):

- `OLLAMA_MODEL` — название модели Ollama (по умолчанию `qwen2.5:7b`)
  - Примеры: `qwen2.5:7b`, `llama3.1:8b`, `mistral:7b`
  - Убедитесь, что модель загружена: `ollama pull <model_name>`
- `OLLAMA_BASE_URL` — адрес Ollama сервера (по умолчанию `http://localhost:11434`)
  - Для удалённого сервера укажите полный URL

Модель эмбеддингов: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (локально, без сети после первого скачивания).

### Гарантия «только по материалам»
- Ретривер извлекает релевантные фрагменты из векторного индекса, а промпты жёстко ограничивают ответ содержимым извлечённых фрагментов. Если ответа нет — агент честно сообщает об этом.

### Структура
```
src/
  config.py        # загрузка .env, настройка LLM/эмбеддингов
  ingest.py        # чтение PDF/DOCX, разбиение, эмбеддинги, построение FAISS
  vectordb.py      # обёртка над FAISS + сохранение/загрузка
  llm.py           # провайдер Ollama
  rag.py           # QA-цепочка с ограничителями
  quiz.py          # генерация квизов (проверка знаний)
  tasks.py         # генерация заданий
  server.py        # FastAPI
  cli.py           # CLI интерфейс
vector_store/      # файлы индекса FAISS
data/              # ваши лекции (.pdf/.docx)
```

### Деплой на сервер
1) Скопируйте проект на сервер.
2) Установите зависимости и переменные окружения.
3) Выполните `python -m src.cli ingest`.
4) Запустите API: `uvicorn src.server:app --host 0.0.0.0 --port 8000`.