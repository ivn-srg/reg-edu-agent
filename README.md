## RAG-EDU Agent (Local)

Локальный RAG-агент для курса «Хранение данных и Введение в Машинное обучение».
Агент отвечает ТОЛЬКО по материалам в папке `data/`, умеет:
- отвечать на вопросы в рамках материалов;
- задавать вопросы для проверки знаний (квизы);
- составлять задания по обсуждаемой теме;
— все строго в рамках предоставленных материалов (DOCX/PDF).

### Быстрый старт
1) Подготовьте Python 3.10+ и виртуальное окружение.
2) Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```
3) Поместите лекции в `data/` (форматы: `.pdf`, `.docx`).
4) Постройте векторное хранилище:
   ```bash
   python -m src.cli ingest
   ```
5) Задайте вопрос:
   ```bash
   python -m src.cli ask "Что такое нормализация данных?"
   ```
6) Сгенерируйте квиз по теме:
   ```bash
   python -m src.cli quiz --topic "Хранение данных" --num 5
   ```
7) Сгенерируйте задание:
   ```bash
   python -m src.cli task --topic "Введение в Машинное обучение"
   ```

### Запуск локального API
```bash
uvicorn src.server:app --host 0.0.0.0 --port 8000
```

Эндпоинты:
- `POST /ingest` — перестроить индекс из `data/`
- `POST /ask` — вопрос-ответ по материалам
- `POST /quiz` — генерация квиза
- `POST /task` — генерация задания

### Конфигурация LLM
По умолчанию используется LLM через OpenAI-совместимый эндпоинт или Ollama.
Задайте переменные окружения (создайте `.env` при необходимости):

- `LLM_PROVIDER` = `openai` или `ollama`
- для OpenAI-совместимого:
  - `OPENAI_API_KEY`
  - при необходимости `OPENAI_BASE_URL` (например для LM Studio/LocalAI)
- для Ollama:
  - `OLLAMA_MODEL` (например, `qwen2.5:7b` или `llama3.1:8b`)

Модель эмбеддингов: `sentence-transformers/all-MiniLM-L6-v2` (локально, без сети после первого скачивания).

### Гарантия «только по материалам»
- Ретривер извлекает релевантные фрагменты из векторного индекса, а промпты жёстко ограничивают ответ содержимым извлечённых фрагментов. Если ответа нет — агент честно сообщает об этом.

### Структура
```
src/
  config.py        # загрузка .env, выбор LLM/эмбеддингов
  ingest.py        # чтение PDF/DOCX, разбиение, эмбеддинги, построение FAISS
  vectordb.py      # обёртка над FAISS + сохранение/загрузка
  llm.py           # провайдеры LLM (OpenAI-совм., Ollama)
  rag.py           # QA-цепочка с ограничителями
  quiz.py          # генерация квизов (проверка знаний)
  tasks.py         # генерация заданий
  server.py        # FastAPI
  cli.py           # CLI интерфейс
vector_store/      # файлы индекса FAISS
data/              # ваши лекции (.pdf/.docx)
```

### Деплой на сервер
1) Скопируйте проект на сервер.
2) Установите зависимости и переменные окружения.
3) Выполните `python -m src.cli ingest`.
4) Запустите API: `uvicorn src.server:app --host 0.0.0.0 --port 8000`.

### Лицензии
Проверьте лицензии используемых моделей LLM/эмбеддингов перед продакшеном.

