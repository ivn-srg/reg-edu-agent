## RAG-EDU Agent (Local)

Локальный RAG-агент для курса «Хранение данных и Введение в Машинное обучение».
Агент отвечает ТОЛЬКО по материалам в папке `data/`, умеет:
- отвечать на вопросы в рамках материалов;
- задавать вопросы для проверки знаний (квизы);
- составлять задания по обсуждаемой теме;
— все строго в рамках предоставленных материалов (DOCX/PDF).

### Быстрый старт
1) Подготовьте Python 3.10+ и виртуальное окружение.
2) Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```
3) Поместите лекции в `data/` (форматы: `.pdf`, `.docx`).
4) Постройте векторное хранилище:
   ```bash
   python -m src.cli ingest
   ```
5) Задайте вопрос:
   ```bash
   python -m src.cli ask "Что такое нормализация данных?"
   ```
6) Сгенерируйте квиз по теме:
   ```bash
   python -m src.cli quiz --topic "Хранение данных" --num 5
   ```
7) Сгенерируйте задание:
   ```bash
   python -m src.cli task --topic "Введение в Машинное обучение"
   ```

### Запуск локального API
```bash
uvicorn src.server:app --host 0.0.0.0 --port 8000
```

Эндпоинты:
- `POST /ingest` — перестроить индекс из `data/`
- `POST /ask` — вопрос-ответ по материалам
- `POST /quiz` — генерация квиза
- `POST /task` — генерация задания

### Конфигурация LLM
По умолчанию используется Ollama.
Задайте переменные окружения (создайте `.env` при необходимости):

- `OLLAMA_MODEL` — название модели Ollama (по умолчанию `qwen2.5:7b`)
  - Примеры: `qwen2.5:7b`, `llama3.1:8b`, `mistral:7b`
  - Убедитесь, что модель загружена: `ollama pull <model_name>`
- `OLLAMA_BASE_URL` — адрес Ollama сервера (по умолчанию `http://localhost:11434`)
  - Для удалённого сервера укажите полный URL

Модель эмбеддингов: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (локально, без сети после первого скачивания).

### Гарантия «только по материалам»
- Ретривер извлекает релевантные фрагменты из векторного индекса, а промпты жёстко ограничивают ответ содержимым извлечённых фрагментов. Если ответа нет — агент честно сообщает об этом.

### Структура
```
src/
  config.py        # загрузка .env, настройка LLM/эмбеддингов
  ingest.py        # чтение PDF/DOCX, разбиение, эмбеддинги, построение FAISS
  vectordb.py      # обёртка над FAISS + сохранение/загрузка
  llm.py           # провайдер Ollama
  rag.py           # QA-цепочка с ограничителями
  quiz.py          # генерация квизов (проверка знаний)
  tasks.py         # генерация заданий
  server.py        # FastAPI
  cli.py           # CLI интерфейс
vector_store/      # файлы индекса FAISS
data/              # ваши лекции (.pdf/.docx)
```

### Деплой на сервер
1) Скопируйте проект на сервер.
2) Установите зависимости и переменные окружения.
3) Выполните `python -m src.cli ingest`.
4) Запустите API: `uvicorn src.server:app --host 0.0.0.0 --port 8000`.

### Лицензии
Проверьте лицензии используемых моделей LLM/эмбеддингов перед продакшеном.

